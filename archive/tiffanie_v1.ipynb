{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Environment setup and local dependencies\n",
    "We install from local `requirements.txt`, load `.env` if present, and ensure the project path is importable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment and dependencies\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "project_root = Path(r\"C:\\Users\\carry\\OneDrive\\Ð”Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ñ‹\\Cursor\\LangChain\\hna\")\n",
    "requirements_path = project_root / \"requirements.txt\"\n",
    "\n",
    "if requirements_path.exists():\n",
    "    import subprocess\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-r\", str(requirements_path)])\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "for env_candidate in [project_root / \".env\", Path.cwd() / \".env\"]:\n",
    "    if env_candidate.exists():\n",
    "        load_dotenv(str(env_candidate))\n",
    "        break\n",
    "\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.append(str(project_root))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Centralized prompts (system and human)\n",
    "We keep prompts versioned and easily extendable. Adjust these variables as the scope grows.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompts (modifiable)\n",
    "from datetime import datetime\n",
    "\n",
    "DISCLAIMER_TEXT = (\n",
    "    \"This conversation is for education only and is not medical advice. \"\n",
    "    \"If you have urgent symptoms, seek professional care or emergency services.\"\n",
    ")\n",
    "\n",
    "SYSTEM_PROMPT = f\"\"\"\n",
    "You are a compassionate multi-turn health navigator.\n",
    "- Greet empathetically and acknowledge feelings.\n",
    "- Ask concise follow-ups to gather symptoms, onset, severity, and risk factors.\n",
    "- Use retrieved WHO/CDC guidance for education only.\n",
    "- Never provide diagnosis or treatment instructions.\n",
    "- Always respect the user's autonomy and privacy.\n",
    "- When flagged, prepend the disclaimer:\n",
    "  \"{DISCLAIMER_TEXT}\"\n",
    "\"\"\"\n",
    "\n",
    "HUMAN_PROMPT_TEMPLATE = (\n",
    "    \"User message: {{user_input}}\\n\"\n",
    "    \"Known symptoms so far: {{symptom_state}}\\n\"\n",
    "    \"Retrieved guidance (if any): {{retrieved_context}}\\n\"\n",
    "    \"Respond empathetically and continue the dialogue.\"\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prompts loaded. You can edit SYSTEM_PROMPT and HUMAN_PROMPT_TEMPLATE above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Load WHO/CDC guidance (web scraping)\n",
    "We fetch content from `https://www.who.int/health-topics` and `https://www.cdc.gov/health-topics.html` and parse text for RAG.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WHO text length: 8199\n",
      "CDC text length: 506\n"
     ]
    }
   ],
   "source": [
    "# Simple web scraper for WHO/CDC health topics\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "WHO_URL = \"https://www.who.int/health-topics\"\n",
    "CDC_URL = \"https://www.cdc.gov/health-topics.html\"\n",
    "\n",
    "\n",
    "def fetch_text_from_url(url: str) -> str:\n",
    "    try:\n",
    "        resp = requests.get(url, timeout=20)\n",
    "        resp.raise_for_status()\n",
    "        soup = BeautifulSoup(resp.text, \"lxml\")\n",
    "        # Keep visible text within main content areas; fallback to body text\n",
    "        main = soup.find(\"main\") or soup\n",
    "        texts = [t.get_text(\" \", strip=True) for t in main.find_all([\"p\", \"li\", \"h1\", \"h2\", \"h3\"])[:2000]]\n",
    "        return \"\\n\".join(t for t in texts if t)\n",
    "    except Exception as e:\n",
    "        return f\"ERROR fetching {url}: {e}\"\n",
    "\n",
    "who_text = fetch_text_from_url(WHO_URL)\n",
    "cdc_text = fetch_text_from_url(CDC_URL)\n",
    "\n",
    "print(\"WHO text length:\", len(who_text))\n",
    "print(\"CDC text length:\", len(cdc_text))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search JSON URL for providers by list of target zips within X mile radius of current zipcode and returns top 10 providers\n",
    "### fetch JSON url from list of providers by state: https://www22.anthem.com/cms-data-index.json/index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Zip codes within 10 miles of 95008:\n",
      "  95008: 0.00 miles\n",
      "  95009: 0.73 miles\n",
      "  95011: 0.73 miles\n",
      "  95130: 1.24 miles\n",
      "  95124: 2.37 miles\n",
      "  95117: 2.38 miles\n",
      "  95128: 2.80 miles\n",
      "  95129: 3.19 miles\n",
      "  95125: 3.62 miles\n",
      "  95071: 3.74 miles\n",
      "  95031: 3.75 miles\n",
      "  95126: 3.87 miles\n",
      "  95118: 4.04 miles\n",
      "  95030: 4.06 miles\n",
      "  95032: 4.42 miles\n",
      "  95053: 4.97 miles\n",
      "  95050: 5.01 miles\n",
      "  95052: 5.06 miles\n",
      "  95055: 5.06 miles\n",
      "  95056: 5.06 miles\n",
      "\n",
      "Total: 75 zip codes found\n",
      "\n",
      "Found 1358 providers in the target ZIP codes.\n",
      "\n",
      "{\n",
      "    \"npi\": \"1003008772\",\n",
      "    \"type\": \"INDIVIDUAL\",\n",
      "    \"name\": \"LOUISE M BRUCE\",\n",
      "    \"address\": \"750 S BASCOM AVE STE 330\",\n",
      "    \"city\": \"SAN JOSE\",\n",
      "    \"state\": \"CA\",\n",
      "    \"zip\": \"95128\",\n",
      "    \"phone\": \"8883341000\",\n",
      "    \"specialty\": \"Nurse Practitioner\"\n",
      "}\n",
      "{\n",
      "    \"npi\": \"1003013715\",\n",
      "    \"type\": \"INDIVIDUAL\",\n",
      "    \"name\": \"ROBERTA Y WANG\",\n",
      "    \"address\": \"751 S BASCOM AVE BLDG Q FL 1\",\n",
      "    \"city\": \"SAN JOSE\",\n",
      "    \"state\": \"CA\",\n",
      "    \"zip\": \"95128\",\n",
      "    \"phone\": \"4088852100\",\n",
      "    \"specialty\": \"Physical Medicine And Rehab\"\n",
      "}\n",
      "{\n",
      "    \"npi\": \"1003017252\",\n",
      "    \"type\": \"FACILITY\",\n",
      "    \"name\": null,\n",
      "    \"address\": \"5150 GRAVES AVE STE 6\",\n",
      "    \"city\": \"SAN JOSE\",\n",
      "    \"state\": \"CA\",\n",
      "    \"zip\": \"95129\",\n",
      "    \"phone\": \"4083660890\",\n",
      "    \"specialty\": null\n",
      "}\n",
      "{\n",
      "    \"npi\": \"1003025586\",\n",
      "    \"type\": \"INDIVIDUAL\",\n",
      "    \"name\": \"KARENA C PETERSEN\",\n",
      "    \"address\": \"137 E HAMILTON AVE STE 204\",\n",
      "    \"city\": \"CAMPBELL\",\n",
      "    \"state\": \"CA\",\n",
      "    \"zip\": \"95008\",\n",
      "    \"phone\": \"4087720644\",\n",
      "    \"specialty\": \"Marriage/Family Therapist\"\n",
      "}\n",
      "{\n",
      "    \"npi\": \"1003045683\",\n",
      "    \"type\": \"INDIVIDUAL\",\n",
      "    \"name\": \"KALI R TILESTON\",\n",
      "    \"address\": \"751 S BASCOM AVE # Q STE 240\",\n",
      "    \"city\": \"SAN JOSE\",\n",
      "    \"state\": \"CA\",\n",
      "    \"zip\": \"95128\",\n",
      "    \"phone\": \"4087932520\",\n",
      "    \"specialty\": \"Surgery, Orthopedic\"\n",
      "}\n",
      "{\n",
      "    \"npi\": \"1003059395\",\n",
      "    \"type\": \"INDIVIDUAL\",\n",
      "    \"name\": \"QUAZI Z AL-TARIQ\",\n",
      "    \"address\": \"225 N JACKSON AVE\",\n",
      "    \"city\": \"SAN JOSE\",\n",
      "    \"state\": \"CA\",\n",
      "    \"zip\": \"95116\",\n",
      "    \"phone\": \"4082595000\",\n",
      "    \"specialty\": \"Diagnostic Radiology\"\n",
      "}\n",
      "{\n",
      "    \"npi\": \"1003141102\",\n",
      "    \"type\": \"INDIVIDUAL\",\n",
      "    \"name\": \"KHOI D NGUYEN\",\n",
      "    \"address\": \"1693 FLANIGAN DR STE 101\",\n",
      "    \"city\": \"SAN JOSE\",\n",
      "    \"state\": \"CA\",\n",
      "    \"zip\": \"95121\",\n",
      "    \"phone\": \"4082703595\",\n",
      "    \"specialty\": \"Family Practice\"\n",
      "}\n",
      "{\n",
      "    \"npi\": \"1003158544\",\n",
      "    \"type\": \"INDIVIDUAL\",\n",
      "    \"name\": \"AMANDEEP SINGH SANDHU\",\n",
      "    \"address\": \"200 JOSE FIGUERES AVE STE 465\",\n",
      "    \"city\": \"SAN JOSE\",\n",
      "    \"state\": \"CA\",\n",
      "    \"zip\": \"95116\",\n",
      "    \"phone\": \"4087295488\",\n",
      "    \"specialty\": \"Dermatology\"\n",
      "}\n",
      "{\n",
      "    \"npi\": \"1003226564\",\n",
      "    \"type\": \"INDIVIDUAL\",\n",
      "    \"name\": \"RIPAL K SHAH\",\n",
      "    \"address\": \"2400 MOORPARK AVE FL 3 STE 316B\",\n",
      "    \"city\": \"SAN JOSE\",\n",
      "    \"state\": \"CA\",\n",
      "    \"zip\": \"95128\",\n",
      "    \"phone\": \"4088855935\",\n",
      "    \"specialty\": \"Psychiatry\"\n",
      "}\n",
      "{\n",
      "    \"npi\": \"1003824699\",\n",
      "    \"type\": \"INDIVIDUAL\",\n",
      "    \"name\": \"KOMAL J BHATT\",\n",
      "    \"address\": \"200 JOSE FIGUERES AVE STE 490\",\n",
      "    \"city\": \"SAN JOSE\",\n",
      "    \"state\": \"CA\",\n",
      "    \"zip\": \"95116\",\n",
      "    \"phone\": \"4082585083\",\n",
      "    \"specialty\": \"Internal Medicine\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "from geopy.distance import geodesic\n",
    "from typing import List\n",
    "import zipcodes\n",
    "\n",
    "def get_zip_codes_within_distance(target_zip: str, radius_miles: float, debug=False) -> List[str]:\n",
    "    \"\"\"\n",
    "    Returns a list of zip codes within X miles from a target zip code.\n",
    "    Uses bounding box optimization and CA-only filtering for speed.\n",
    "    \n",
    "    Args:\n",
    "        target_zip: The target zip code (string)\n",
    "        radius_miles: Search radius in miles\n",
    "    \n",
    "    Returns:\n",
    "        List of zip codes within the specified radius\n",
    "    \"\"\"\n",
    "    import math\n",
    "    \n",
    "    # Get target zip code info\n",
    "    target_info = zipcodes.matching(target_zip)\n",
    "    if not target_info:\n",
    "        raise ValueError(f\"Could not find zip code '{target_zip}'\")\n",
    "    \n",
    "    target_lat = float(target_info[0]['lat'])\n",
    "    target_lon = float(target_info[0]['long'])\n",
    "    target_coords = (target_lat, target_lon)\n",
    "    \n",
    "    # Calculate bounding box\n",
    "    lat_offset = radius_miles / 69\n",
    "    lon_offset = radius_miles / (69 * math.cos(math.radians(target_lat)))\n",
    "    \n",
    "    min_lat = target_lat - lat_offset\n",
    "    max_lat = target_lat + lat_offset\n",
    "    min_lon = target_lon - lon_offset\n",
    "    max_lon = target_lon + lon_offset\n",
    "    \n",
    "    nearby_zips_with_distance = []\n",
    "    \n",
    "    # Add the target zip first\n",
    "    nearby_zips_with_distance.append((target_zip, 0))\n",
    "    \n",
    "    # Only check California zip codes within bounding box\n",
    "    for zip_obj in zipcodes.list_all():\n",
    "        zip_code = zip_obj['zip_code']\n",
    "        \n",
    "        # Skip the target zip since we already added it\n",
    "        if zip_code == target_zip:\n",
    "            continue\n",
    "            \n",
    "        # Only include California zip codes\n",
    "        if zip_obj['state'] != 'CA':\n",
    "            continue\n",
    "        \n",
    "        lat = float(zip_obj['lat'])\n",
    "        lon = float(zip_obj['long'])\n",
    "        \n",
    "        # Quick bounding box check first\n",
    "        if lat < min_lat or lat > max_lat or lon < min_lon or lon > max_lon:\n",
    "            continue\n",
    "        \n",
    "        # Only calculate exact distance for candidates in bounding box\n",
    "        coords = (lat, lon)\n",
    "        distance = geodesic(target_coords, coords).miles\n",
    "        \n",
    "        if distance <= radius_miles:\n",
    "            nearby_zips_with_distance.append((zip_code, distance))\n",
    "    \n",
    "    # Sort by distance (nearest to farthest)\n",
    "    nearby_zips_with_distance.sort(key=lambda x: x[1])\n",
    "    \n",
    "    # Debug output if requested\n",
    "    if debug:\n",
    "        print(f\"\\nZip codes within {radius_miles} miles of {target_zip}:\")\n",
    "        for zip_code, distance in nearby_zips_with_distance[:20]:\n",
    "            print(f\"  {zip_code}: {distance:.2f} miles\")\n",
    "    \n",
    "    # Return just the zip codes\n",
    "    return [zip_code for zip_code, distance in nearby_zips_with_distance]\n",
    "\n",
    "def scrape_json_url(url):\n",
    "    \"\"\"Fetch JSON from a given URL.\"\"\"\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        return response.json()\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching data from {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def filter_providers_by_zip(url, target_zips):\n",
    "    \"\"\"\n",
    "    Fetch providers from Anthem JSON URL and return filtered provider data\n",
    "    for a list of ZIP codes (\"addresses\" -> \"zip\").\n",
    "    \n",
    "    Args:\n",
    "        url: The URL to fetch provider data from\n",
    "        target_zips: A string (single ZIP) or list of strings (multiple ZIPs)\n",
    "    \n",
    "    Returns:\n",
    "        List of provider dictionaries matching the target ZIPs\n",
    "    \"\"\"\n",
    "    data = scrape_json_url(url)\n",
    "    if not data:\n",
    "        return []\n",
    "\n",
    "    if not isinstance(data, list):\n",
    "        print(\"Unexpected JSON structure.\")\n",
    "        return []\n",
    "\n",
    "    # Convert target_zips to list if it's a string\n",
    "    if isinstance(target_zips, str):\n",
    "        target_zips = [target_zips]\n",
    "    \n",
    "    # Normalize all target zips\n",
    "    target_zips = [str(z).strip() for z in target_zips]\n",
    "\n",
    "    filtered = []\n",
    "\n",
    "    for p in data:\n",
    "        addresses = p.get(\"addresses\", [])\n",
    "        if not addresses:\n",
    "            continue\n",
    "\n",
    "        # Some providers may have multiple addresses\n",
    "        for addr in addresses:\n",
    "            zip_code = str(addr.get(\"zip\", \"\")).strip()\n",
    "            if zip_code in target_zips:\n",
    "                name_info = p.get(\"name\", {})\n",
    "                # Flatten name dict if present\n",
    "                full_name = \" \".join(\n",
    "                    filter(None, [name_info.get(\"first\"), name_info.get(\"middle\"), name_info.get(\"last\")])\n",
    "                ).strip()\n",
    "\n",
    "                # Specialty is a list â€” join into string\n",
    "                specialty = \", \".join(p.get(\"specialty\", [])) if isinstance(p.get(\"specialty\"), list) else p.get(\"specialty\")\n",
    "\n",
    "                result = {\n",
    "                    \"npi\": p.get(\"npi\"),\n",
    "                    \"type\": p.get(\"type\"),\n",
    "                    \"name\": full_name or p.get(\"name\"),\n",
    "                    \"address\": addr.get(\"address\"),\n",
    "                    \"city\": addr.get(\"city\"),\n",
    "                    \"state\": addr.get(\"state\"),\n",
    "                    \"zip\": zip_code,\n",
    "                    \"phone\": addr.get(\"phone\"),\n",
    "                    \"specialty\": specialty\n",
    "                }\n",
    "                filtered.append(result)\n",
    "                break  # Stop after first matching address for this provider\n",
    "\n",
    "    return filtered\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    target_zips = get_zip_codes_within_distance('95008', 10, debug=True)\n",
    "    print(f\"\\nTotal: {len(target_zips)} zip codes found\\n\")\n",
    "    \n",
    "    json_url = \"https://www22.anthem.com/CMS/PROVIDERS_CAM.json\"\n",
    "    results = filter_providers_by_zip(json_url, target_zips)\n",
    "\n",
    "    print(f\"Found {len(results)} providers in the target ZIP codes.\\n\")\n",
    "    for r in results[:10]:\n",
    "        print(json.dumps(r, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Vector store with Chroma and embeddings\n",
    "We chunk scraped text and store embeddings locally. This will power retrieval.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build vector store\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "raw_docs = [\n",
    "    {\"source\": WHO_URL, \"text\": who_text},\n",
    "    {\"source\": CDC_URL, \"text\": cdc_text},\n",
    "]\n",
    "\n",
    "# Basic cleaning and chunking\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=800, chunk_overlap=150)\n",
    "\n",
    "documents = []\n",
    "for d in raw_docs:\n",
    "    if not d[\"text\"].startswith(\"ERROR\"):\n",
    "        for chunk in splitter.split_text(d[\"text\"]):\n",
    "            documents.append({\"page_content\": chunk, \"metadata\": {\"source\": d[\"source\"]}})\n",
    "\n",
    "# Embeddings model\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# Create Chroma DB in a local folder\n",
    "chroma_dir = str(project_root / \"chroma_db\")\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=[\n",
    "        type(\"Doc\", (), doc)() for doc in documents  # lightweight doc-like objects\n",
    "    ],\n",
    "    embedding=embeddings,\n",
    "    persist_directory=chroma_dir,\n",
    ")\n",
    "\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 4})\n",
    "print(\"Vector DB ready. Docs:\", len(documents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5) LangChain retrieval stage\n",
    "We use the retriever to ground the response. This is still LangChain.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\carry\\AppData\\Local\\Temp\\ipykernel_12664\\1520874221.py:13: LangChainDeprecationWarning: The class `HuggingFaceHub` was deprecated in LangChain 0.0.21 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEndpoint``.\n",
      "  llm = HuggingFaceHub(\n"
     ]
    }
   ],
   "source": [
    "# LangChain retrieval\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema import StrOutputParser\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "from langchain.llms import HuggingFaceHub\n",
    "import os\n",
    "\n",
    "# Set your Hugging Face token\n",
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"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\"\n",
    "\n",
    "# Use a model from Hugging Face Hub\n",
    "llm = HuggingFaceHub(\n",
    "    repo_id=\"mistralai/Mistral-7B-Instruct-v0.1\", # Mistral 7B - Excellent for Reasoning, balance of performance and efficiency\n",
    "    model_kwargs={\n",
    "        \"temperature\": 0.7, \n",
    "        \"max_length\": 512,  # Increased for better responses\n",
    "        \"top_p\": 0.95,\n",
    "        \"repetition_penalty\": 1.1\n",
    "    }\n",
    ")\n",
    "\n",
    "RAG_TEMPLATE = ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    {system}\n",
    "    \n",
    "    Context:\n",
    "    {context}\n",
    "    \n",
    "    Conversation state:\n",
    "    - Symptoms: {symptom_state}\n",
    "    \n",
    "    User: {user_input}\n",
    "    Assistant: Provide an empathetic, educational reply. Do not diagnose or prescribe.\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "chain = (\n",
    "    RAG_TEMPLATE\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Helper to run a retrieved response\n",
    "def retrieved_response(user_input: str, symptom_state: str = \"\"):\n",
    "    docs = retriever.get_relevant_documents(user_input)\n",
    "    context = \"\\n\\n\".join(d.page_content for d in docs)\n",
    "    return chain.invoke({\n",
    "        \"system\": SYSTEM_PROMPT,\n",
    "        \"context\": context,\n",
    "        \"symptom_state\": symptom_state,\n",
    "        \"user_input\": user_input,\n",
    "    })\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LangChain retrieval chain ready"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6) SWITCH: LangChain â†’ LangGraph\n",
    "We now orchestrate the multi-turn dialogue using LangGraph nodes and edges. Vector DB is used inside the retrieval node.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LangGraph workflow\n",
    "from typing import TypedDict, Annotated\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "# Define proper state schema for LangGraph\n",
    "class State(TypedDict):\n",
    "    user_input: str\n",
    "    symptom_state: str\n",
    "    retrieved_context: str\n",
    "    risk_level: str\n",
    "    response_text: str\n",
    "    empathy: bool\n",
    "\n",
    "# Nodes\n",
    "\n",
    "def node_empathy(state: State) -> State:\n",
    "    return {**state, \"empathy\": True}\n",
    "\n",
    "def node_collect_symptoms(state: State) -> State:\n",
    "    user_input = state.get(\"user_input\", \"\")\n",
    "    # naive extraction heuristic; plug in a structured symptom extractor later\n",
    "    known = state.get(\"symptom_state\", \"\")\n",
    "    new_symptom_state = (known + \" \" + user_input).strip()\n",
    "    return {**state, \"symptom_state\": new_symptom_state}\n",
    "\n",
    "def node_retrieve(state: State) -> State:\n",
    "    query = state.get(\"user_input\", \"\")\n",
    "    docs = retriever.get_relevant_documents(query)\n",
    "    retrieved_context = \"\\n\\n\".join(d.page_content for d in docs)\n",
    "    return {**state, \"retrieved_context\": retrieved_context}\n",
    "\n",
    "def node_risk_assess(state: State) -> State:\n",
    "    # placeholder; future: rules or model-based risk heuristics\n",
    "    text = state.get(\"symptom_state\", \"\")\n",
    "    risk = \"low\"\n",
    "    if any(k in text.lower() for k in [\"chest pain\", \"shortness of breath\", \"fainting\"]):\n",
    "        risk = \"high\"\n",
    "    return {**state, \"risk_level\": risk}\n",
    "\n",
    "def node_respond(state: State) -> State:\n",
    "    response = retrieved_response(\n",
    "        user_input=state.get(\"user_input\", \"\"),\n",
    "        symptom_state=state.get(\"symptom_state\", \"\"),\n",
    "    )\n",
    "    return {**state, \"response_text\": response}\n",
    "\n",
    "# Build graph\n",
    "builder = StateGraph(State)\n",
    "\n",
    "builder.add_node(\"empathy\", node_empathy)\n",
    "builder.add_node(\"collect_symptoms\", node_collect_symptoms)\n",
    "builder.add_node(\"retrieve\", node_retrieve)\n",
    "builder.add_node(\"risk\", node_risk_assess)\n",
    "builder.add_node(\"respond\", node_respond)\n",
    "\n",
    "builder.set_entry_point(\"empathy\")\n",
    "\n",
    "# Linear flow for now; can branch by conditions later\n",
    "builder.add_edge(\"empathy\", \"collect_symptoms\")\n",
    "builder.add_edge(\"collect_symptoms\", \"retrieve\")\n",
    "builder.add_edge(\"retrieve\", \"risk\")\n",
    "builder.add_edge(\"risk\", \"respond\")\n",
    "builder.add_edge(\"respond\", END)\n",
    "\n",
    "memory = MemorySaver()\n",
    "app = builder.compile(checkpointer=memory)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7) Disclaimer filter and sentiment analysis\n",
    "We add a toggle to include the disclaimer and run a lightweight VADER sentiment to adapt tone.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disclaimer + sentiment\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "def apply_disclaimer(text: str, enabled: bool) -> str:\n",
    "    if not enabled:\n",
    "        return text\n",
    "    if DISCLAIMER_TEXT in text:\n",
    "        return text\n",
    "    return f\"{DISCLAIMER_TEXT}\\n\\n{text}\"\n",
    "\n",
    "\n",
    "def adapt_tone_with_sentiment(user_input: str, reply: str) -> str:\n",
    "    scores = analyzer.polarity_scores(user_input)\n",
    "    if scores.get(\"neg\", 0) > 0.4:\n",
    "        # append a brief empathetic line\n",
    "        return reply + \"\\n\\nIâ€™m here with you. That sounds tough.\"\n",
    "    return reply\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Disclaimer and sentiment ready."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8) Gradio UI for Health Navigator\n",
    "Interactive chat interface with disclaimer toggle and conversation history.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "puOmAtTdhSbR"
   },
   "outputs": [],
   "source": [
    "# Gradio UI\n",
    "import gradio as gr\n",
    "from typing import List, Tuple\n",
    "\n",
    "# Global conversation state\n",
    "conversation_history = []\n",
    "\n",
    "def chat_with_agent(message: str, history: List[Tuple[str, str]], disclaimer_enabled: bool) -> Tuple[str, List[Tuple[str, str]]]:\n",
    "    \"\"\"Main chat function that processes user input through the LangGraph workflow\"\"\"\n",
    "    if not message.strip():\n",
    "        return \"\", history\n",
    "    \n",
    "    try:\n",
    "        # Run through LangGraph workflow\n",
    "        config = {\"configurable\": {\"thread_id\": \"main\"}}\n",
    "        result = app.invoke(\n",
    "            {\"user_input\": message, \"symptom_state\": \"\"}, \n",
    "            config=config\n",
    "        )\n",
    "        \n",
    "        # Get the response\n",
    "        response = result.get(\"response_text\", \"I'm sorry, I couldn't process that.\")\n",
    "        \n",
    "        # Apply sentiment adaptation\n",
    "        response = adapt_tone_with_sentiment(message, response)\n",
    "        \n",
    "        # Apply disclaimer if enabled\n",
    "        response = apply_disclaimer(response, disclaimer_enabled)\n",
    "        \n",
    "        # Update history\n",
    "        history.append((message, response))\n",
    "        \n",
    "        return \"\", history\n",
    "        \n",
    "    except Exception as e:\n",
    "        error_msg = f\"I encountered an error: {str(e)}. Please try again.\"\n",
    "        history.append((message, error_msg))\n",
    "        return \"\", history\n",
    "\n",
    "def clear_history():\n",
    "    \"\"\"Clear conversation history\"\"\"\n",
    "    global conversation_history\n",
    "    conversation_history = []\n",
    "    return []\n",
    "\n",
    "# Create Gradio interface\n",
    "with gr.Blocks(title=\"Health Navigator Agent\", theme=gr.themes.Soft()) as demo:\n",
    "    gr.Markdown(\"\"\"\n",
    "    # ðŸ¥ Health Navigator Agent\n",
    "    \n",
    "    A compassionate AI assistant that helps you understand health topics using WHO/CDC guidance.\n",
    "    \n",
    "    **âš ï¸ Important:** This is for educational purposes only and does not provide medical advice.\n",
    "    \"\"\")\n",
    "    \n",
    "    with gr.Row():\n",
    "        with gr.Column(scale=3):\n",
    "            chatbot = gr.Chatbot(\n",
    "                value=[],\n",
    "                height=500,\n",
    "                label=\"Conversation\",\n",
    "                show_label=True\n",
    "            )\n",
    "            \n",
    "            with gr.Row():\n",
    "                msg_input = gr.Textbox(\n",
    "                    placeholder=\"Describe your symptoms or ask a health question...\",\n",
    "                    label=\"Your message\",\n",
    "                    lines=2,\n",
    "                    scale=4\n",
    "                )\n",
    "                send_btn = gr.Button(\"Send\", variant=\"primary\", scale=1)\n",
    "            \n",
    "            with gr.Row():\n",
    "                disclaimer_toggle = gr.Checkbox(\n",
    "                    label=\"Include medical disclaimer\",\n",
    "                    value=True,\n",
    "                    info=\"Adds safety disclaimer to responses\"\n",
    "                )\n",
    "                clear_btn = gr.Button(\"Clear History\", variant=\"secondary\")\n",
    "        \n",
    "        with gr.Column(scale=1):\n",
    "            gr.Markdown(\"\"\"\n",
    "            ### Features\n",
    "            - ðŸ¤— Empathetic responses\n",
    "            - ðŸ“š WHO/CDC guidance retrieval\n",
    "            - ðŸ§  Multi-turn conversation\n",
    "            - âš ï¸ Safety disclaimers\n",
    "            - ðŸ’­ Sentiment analysis\n",
    "            - ðŸ”„ LangGraph workflow\n",
    "            \"\"\")\n",
    "            \n",
    "            gr.Markdown(\"\"\"\n",
    "            ### How it works\n",
    "            1. **Empathy**: Acknowledges your feelings\n",
    "            2. **Symptom Collection**: Gathers information\n",
    "            3. **Retrieval**: Finds relevant guidance\n",
    "            4. **Risk Assessment**: Evaluates urgency\n",
    "            5. **Response**: Provides educational support\n",
    "            \"\"\")\n",
    "    \n",
    "    # Event handlers\n",
    "    send_btn.click(\n",
    "        chat_with_agent,\n",
    "        inputs=[msg_input, chatbot, disclaimer_toggle],\n",
    "        outputs=[msg_input, chatbot]\n",
    "    )\n",
    "    \n",
    "    msg_input.submit(\n",
    "        chat_with_agent,\n",
    "        inputs=[msg_input, chatbot, disclaimer_toggle],\n",
    "        outputs=[msg_input, chatbot]\n",
    "    )\n",
    "    \n",
    "    clear_btn.click(\n",
    "        clear_history,\n",
    "        outputs=[chatbot]\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Gradio UI ready! Run demo.launch() to start the interface."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9) Launch and Test the Health Navigator\n",
    "Run the Gradio interface to test the complete experience.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10) Test Hugging Face Model Performance\n",
    "Let's test the model with a sample health question to verify performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model performance\n",
    "print(\"ðŸ§ª Testing Hugging Face model performance...\")\n",
    "\n",
    "# Test 1: Simple health question\n",
    "test_input = \"I have a headache and feel tired. What should I know?\"\n",
    "print(f\"\\nðŸ“ Test Input: {test_input}\")\n",
    "\n",
    "try:\n",
    "    # Test the LangGraph workflow\n",
    "    config = {\"configurable\": {\"thread_id\": \"test\"}}\n",
    "    result = app.invoke({\"user_input\": test_input}, config=config)\n",
    "    \n",
    "    print(\"âœ… LangGraph workflow completed successfully!\")\n",
    "    print(f\"ðŸ“Š Risk Level: {result.get('risk_level', 'unknown')}\")\n",
    "    print(f\"ðŸ’¬ Response Length: {len(result.get('response_text', ''))} characters\")\n",
    "    print(f\"ðŸ” Retrieved Context Length: {len(result.get('retrieved_context', ''))} characters\")\n",
    "    \n",
    "    # Show a snippet of the response\n",
    "    response = result.get('response_text', '')\n",
    "    if response:\n",
    "        print(f\"\\nðŸ¤– Model Response Preview:\")\n",
    "        print(response[:200] + \"...\" if len(response) > 200 else response)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error testing model: {e}\")\n",
    "    print(\"This might be due to Hugging Face API limits or model availability.\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Starting Health Navigator Agent...\n",
      "ðŸ“Š Vector DB contains 14 document chunks\n",
      "ðŸ¤– Using Hugging Face model: Mistral-7B-Instruct\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Type Dict cannot be instantiated; use dict() instead",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mðŸ“Š Vector DB contains\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28mlen\u001b[39m(documents), \u001b[33m\"\u001b[39m\u001b[33mdocument chunks\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mðŸ¤– Using Hugging Face model: Mistral-7B-Instruct\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mðŸ”— LangGraph workflow with\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28mlen\u001b[39m(\u001b[43mapp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_graph\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m.nodes), \u001b[33m\"\u001b[39m\u001b[33mnodes\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      7\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m + \u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m*\u001b[32m50\u001b[39m)\n\u001b[32m      8\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mLaunching Gradio interface...\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\carry\\anaconda3\\Lib\\site-packages\\langgraph\\pregel\\main.py:711\u001b[39m, in \u001b[36mPregel.get_graph\u001b[39m\u001b[34m(self, config, xray)\u001b[39m\n\u001b[32m    708\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    709\u001b[39m     subgraphs = {}\n\u001b[32m--> \u001b[39m\u001b[32m711\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdraw_graph\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    712\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmerge_configs\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    713\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnodes\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnodes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    714\u001b[39m \u001b[43m    \u001b[49m\u001b[43mspecs\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mchannels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    715\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_channels\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minput_channels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    716\u001b[39m \u001b[43m    \u001b[49m\u001b[43minterrupt_after_nodes\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minterrupt_after_nodes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    717\u001b[39m \u001b[43m    \u001b[49m\u001b[43minterrupt_before_nodes\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minterrupt_before_nodes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    718\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrigger_to_nodes\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrigger_to_nodes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    719\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcheckpointer\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcheckpointer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    720\u001b[39m \u001b[43m    \u001b[49m\u001b[43msubgraphs\u001b[49m\u001b[43m=\u001b[49m\u001b[43msubgraphs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    721\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\carry\\anaconda3\\Lib\\site-packages\\langgraph\\pregel\\_draw.py:128\u001b[39m, in \u001b[36mdraw_graph\u001b[39m\u001b[34m(config, nodes, specs, input_channels, interrupt_after_nodes, interrupt_before_nodes, trigger_to_nodes, checkpointer, subgraphs, limit)\u001b[39m\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m task.writers:\n\u001b[32m    125\u001b[39m     \u001b[38;5;66;03m# apply regular writes\u001b[39;00m\n\u001b[32m    126\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(w, ChannelWrite):\n\u001b[32m    127\u001b[39m         empty_input = (\n\u001b[32m--> \u001b[39m\u001b[32m128\u001b[39m             \u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mBaseChannel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mspecs\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m__root__\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mValueType\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    129\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m__root__\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m specs\n\u001b[32m    130\u001b[39m             \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    131\u001b[39m         )\n\u001b[32m    132\u001b[39m         w.invoke(empty_input, task.config)\n\u001b[32m    133\u001b[39m     \u001b[38;5;66;03m# apply conditional writes declared for static analysis, only once\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\carry\\anaconda3\\Lib\\typing.py:1315\u001b[39m, in \u001b[36m_BaseGenericAlias.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1313\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args, **kwargs):\n\u001b[32m   1314\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._inst:\n\u001b[32m-> \u001b[39m\u001b[32m1315\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mType \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m._name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m cannot be instantiated; \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1316\u001b[39m                         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33muse \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.__origin__.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m() instead\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1317\u001b[39m     result = \u001b[38;5;28mself\u001b[39m.__origin__(*args, **kwargs)\n\u001b[32m   1318\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[31mTypeError\u001b[39m: Type Dict cannot be instantiated; use dict() instead"
     ]
    }
   ],
   "source": [
    "# Launch the Gradio interface\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"ðŸš€ Starting Health Navigator Agent...\")\n",
    "    print(\"ðŸ“Š Vector DB contains\", len(documents), \"document chunks\")\n",
    "    print(\"ðŸ¤– Using Hugging Face model: Mistral-7B-Instruct\")\n",
    "    print(\"ðŸ”— LangGraph workflow with\", len(app.get_graph().nodes), \"nodes\")\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"Launching Gradio interface...\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Launch with public sharing disabled for security\n",
    "    demo.launch(\n",
    "        share=False,  # Set to True if you want to share publicly\n",
    "        server_name=\"127.0.0.1\",\n",
    "        server_port=7860,\n",
    "        show_error=True,\n",
    "        quiet=False\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11) Ready to Launch! ðŸš€\n",
    "\n",
    "The Health Navigator Agent is now complete with:\n",
    "\n",
    "âœ… **Fixed Dependencies**: Using local `requirements.txt`  \n",
    "âœ… **Centralized Prompts**: Scalable system and human prompts  \n",
    "âœ… **Web Scraping**: WHO/CDC health guidance loaded  \n",
    "âœ… **Vector Database**: Chroma with sentence-transformers embeddings  \n",
    "âœ… **LangChain Retrieval**: RAG pipeline for context  \n",
    "âœ… **LangGraph Workflow**: Multi-turn dialogue orchestration  \n",
    "âœ… **Disclaimer Filter**: Toggle for safety disclaimers  \n",
    "âœ… **Sentiment Analysis**: VADER for emotional adaptation  \n",
    "âœ… **Gradio UI**: Interactive chat interface  \n",
    "\n",
    "**To run the agent:**\n",
    "1. Execute all cells above\n",
    "2. Run the test cell to verify model performance\n",
    "3. Launch the Gradio interface\n",
    "\n",
    "**Note**: The Hugging Face model (Mistral-7B-Instruct) provides good performance for health conversations. If you encounter API limits, consider using OpenAI or other providers by updating the model configuration in the LangChain retrieval cell.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
