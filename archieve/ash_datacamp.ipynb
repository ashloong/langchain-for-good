{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f989b920",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set Open AI key\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(dotenv_path=\".env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1fa6e64b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_huggingface import HuggingFacePipeline\n",
    "from langchain_core.prompts import PromptTemplate, ChatPromptTemplate, FewShotPromptTemplate\n",
    "from langchain.schema import StrOutputParser\n",
    "from langchain.agents import load_tools, create_react_agent, AgentExecutor\n",
    "from langchain import hub\n",
    "from langchain_core.tools import tool\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6f1c3082",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Define the LLM from the Hugging Face model ID\n",
    "llm = HuggingFacePipeline.from_model_id(\n",
    "    model_id=\"mistralai/Mistral-7B-v0.1\",\n",
    "    task=\"text-generation\",\n",
    "    pipeline_kwargs={\"max_new_tokens\": 200}\n",
    ")\n",
    "\n",
    "prompt = \"What is an MRI?\"\n",
    "response = llm.invoke(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "84121901",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text='Explain this concept simply and concisely: Prompting LLMs'\n"
     ]
    }
   ],
   "source": [
    "template = \"Explain this concept simply and concisely: {concept}\"\n",
    "prompt_template = PromptTemplate.from_template(\n",
    "    template=template\n",
    ")\n",
    "\n",
    "prompt = prompt_template.invoke({\"concept\": \"Prompting LLMs\"})\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232015e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ChatPromptTemplate supports prompting with roles\n",
    "template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a calculator that responds with math.\"),\n",
    "        (\"human\", \"What is 2 plus 2?\"),\n",
    "        (\"ai\", \"2+2=4\"),\n",
    "        (\"human\", \"Solve this math problem: {math}\")\n",
    "    ]\n",
    ")\n",
    "llm_chain = template | llm\n",
    "math='What is five times five?'\n",
    "\n",
    "response = llm_chain.invoke({\"math\": math})\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "546bc408",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FewShotPromptTemplate\n",
    "\n",
    "# Formatting prompt\n",
    "example_prompt = PromptTemplate.from_template(\"{q}\\n{a}\")\n",
    "\n",
    "prompt = example_prompt.invoke({\"q\": \"What's the capital of Italy?\",\n",
    "                                \"a\": \"Rome\"})\n",
    "print(prompt.text)\n",
    "\n",
    "# Few-shot prompt\n",
    "prompt_template = FewShotPromptTemplate(\n",
    "    examples=examples,                  # List of dicts\n",
    "    example_prompt=example_prompt,      # Formatted template\n",
    "    suffix=\"Question: {input}\",         # Format user input\n",
    "    input_variables=[\"input\"]\n",
    ")\n",
    "\n",
    "# Invoke\n",
    "prompt = prompt_template.invoke({\"input\": \"What is the name of Henry Campbell's dog?\"})\n",
    "print(prompt.text)\n",
    "\n",
    "# Integrate with chain\n",
    "llm_chain = prompt_template | llm\n",
    "response = llm_chain.invoke({\"input\": \"What is the name of Barack Obama's dog?\"})\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e8893d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sequential chains\n",
    "\n",
    "# Output -> input\n",
    "destination_prompt = PromptTemplate(\n",
    "    input_variables=[\"destination\"],\n",
    "    template=\"I am planning a trip to {destination}. Can you suggest some activities for me to do there?\"\n",
    ")\n",
    "activities_prompt = PromptTemplate(\n",
    "    input_variables=[\"activities\"],\n",
    "    template=\"I only have one day, so you can create an itinerary from your top 3 activities: {activities}.\"\n",
    ")\n",
    "\n",
    "seq_chain = ({\"activities\": destination_prompt | llm | StrOutputParser()}\n",
    "    | activities_prompt\n",
    "    | llm\n",
    "    | StrOutputParser())\n",
    "\n",
    "print(seq_chain.invoke({\"destination\": \"Costa Rica\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fed92fa",
   "metadata": {},
   "source": [
    "## Langchain Agents\n",
    "\n",
    "In LangChain, agents use language models to determine actions. Agents often use tools, which are functions called by the agent to interact with the system. These tools can be high-level utilities to transform inputs, or they can be task-specific. Agents can even use chains and other agents as tools!\n",
    "\n",
    "### ReAct agents\n",
    "\n",
    "ReAct stands for reasoning and acting, and this is exactly how the agent operates. It prompts the model using a repeated loop of thinking, acting, and observing. If we were to ask a ReAct agent that had access to a weather tool, \"What is the weather like in Kingston, Jamaica?\", it would start by thinking about the task and which tool to call, call that tool using the information, and observe the results from the tool call.\n",
    "\n",
    "### LangGraph\n",
    "\n",
    "To implement agents, we'll be using LangGraph, which is branch of the LangChain ecosystem specifically for designing agentic systems, or systems including agents. Like LangChain's core library, it's is built to provide a unified, tool-agnostic syntax. We'll be using the following version in this course.\n",
    "\n",
    "### Example\n",
    "\n",
    "We'll create a ReAct agent that can solve math problems - something most LLMs struggle with. We import create_react_agent from langgraph and the load_tools() function. We initialize our LLM, and load the llm-math tool using the load_tools() function. To create the agent, we pass the LLM and tools to create_react_agent(), Just like chains, agents can be executed with the .invoke() method. Here, we pass the chat model a message to find the square root of 101, which isn't a whole number. Let's see how the agent approaches the problem!\n",
    "\n",
    "There's a lot of metadata in the output, so we've trimmed it for brevity. We can see that executing the agent resulted in a series of messages. The first is our prompt defining the problem; the second is created by the model to identify the tool to use and to convert our query into mathematical format; the third is the result of the tool call, and the final message is the model's response after observing the tool's answer, which it decided to round to two decimal places. If we just want the final response, we can subset the final message and extract it's content with the .content attribute. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b053563",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ash\\Desktop\\langchain-for-good\\.venv\\Lib\\site-packages\\langsmith\\client.py:241: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
      "  warnings.warn(\n",
      "Error in StdOutCallbackHandler.on_chain_start callback: AttributeError(\"'NoneType' object has no attribute 'get'\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3mTo find the square root of 101, I will use the Calculator tool to perform the calculation. \n",
      "Action: Calculator\n",
      "Action Input: 101**0.5\u001b[0m\u001b[36;1m\u001b[1;3mAnswer: 10.04987562112089\u001b[0m\u001b[32;1m\u001b[1;3mI now know the final answer.  \n",
      "Final Answer: The square root of 101 is approximately 10.05.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "The square root of 101 is approximately 10.05.\n"
     ]
    }
   ],
   "source": [
    "# ReAct agent\n",
    "tools = load_tools([\"llm-math\"], llm=llm)\n",
    "prompt = hub.pull(\"hwchase17/react\")\n",
    "\n",
    "agent = create_react_agent(llm, tools, prompt)\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n",
    "\n",
    "response = agent_executor.invoke({\"input\": \"What is the square root of 101?\"})\n",
    "print(response[\"output\"])\n",
    "# print(response['messages'][-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fe2b6efa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculator\n",
      "Useful for when you need to answer questions about math.\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "# Tool formats\n",
    "print(tools[0].name)\n",
    "print(tools[0].description)\n",
    "print(tools[0].return_direct) # indicates whether the agent should stop after invoking this tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9dc66cab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Financial Report for LemonadeStand:\n",
      "Revenue: $100\n",
      "Expenses: $50\n",
      "Net Income: $50\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define custom function\n",
    "def financial_report(company_name: str, revenue: int, expenses: int) -> str:\n",
    "    \"\"\"Generate a financial report for a company that calculates net income.\"\"\"\n",
    "    net_income = revenue - expenses\n",
    "    \n",
    "    report = f\"Financial Report for {company_name}:\\n\"\n",
    "    report += f\"Revenue: ${revenue}\\n\"\n",
    "    report += f\"Expenses: ${expenses}\\n\"\n",
    "    report += f\"Net Income: ${net_income}\\n\"\n",
    "    return report\n",
    "\n",
    "print(financial_report(company_name=\"LemonadeStand\", revenue=100, expenses=50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f9168966",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "financial_report\n",
      "Generate a financial report for a company that calculates net income.\n",
      "False\n",
      "{'company_name': {'title': 'Company Name', 'type': 'string'}, 'revenue': {'title': 'Revenue', 'type': 'integer'}, 'expenses': {'title': 'Expenses', 'type': 'integer'}}\n"
     ]
    }
   ],
   "source": [
    "# Function -> tool\n",
    "\n",
    "@tool\n",
    "def financial_report(company_name: str, revenue: int, expenses: int) -> str:\n",
    "    \"\"\"Generate a financial report for a company that calculates net income.\"\"\"\n",
    "    net_income = revenue - expenses\n",
    "    \n",
    "    report = f\"Financial Report for {company_name}:\\n\"\n",
    "    report += f\"Revenue: ${revenue}\\n\"\n",
    "    report += f\"Expenses: ${expenses}\\n\"\n",
    "    report += f\"Net Income: ${net_income}\\n\"\n",
    "    return report\n",
    "\n",
    "# Examine\n",
    "print(financial_report.name)\n",
    "print(financial_report.description)\n",
    "print(financial_report.return_direct)\n",
    "print(financial_report.args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9657f821",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Integrate\n",
    "tools = load_tools([\"llm-math\"], llm=llm)\n",
    "prompt = hub.pull(\"hwchase17/react\")\n",
    "\n",
    "agent = create_react_agent(llm, [financial_report], prompt)\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n",
    "\n",
    "messages = agent_executor.invoke({\"input\": \"TechStack generated made $10 million with $8 million of costs. Generate a financial report.\"})\n",
    "print(messages['messages'][-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b44a7e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to retrieve customer info by-name\n",
    "def retrieve_customer_info(name: str) -> str:\n",
    "    \"\"\"Retrieve customer information based on their name.\"\"\"\n",
    "    # Filter customers for the customer's name\n",
    "    customer_info = customers[customers['name'] == name]\n",
    "    return customer_info.to_string()\n",
    "  \n",
    "# Call the function on Peak Performance Co.\n",
    "print(retrieve_customer_info(name=\"Peak Performance Co.\"))\n",
    "\n",
    "@tool\n",
    "def retrieve_customer_info(name: str) -> str:\n",
    "    \"\"\"Retrieve customer information based on their name.\"\"\"\n",
    "    customer_info = customers[customers['name'] == name]\n",
    "    return customer_info.to_string()\n",
    "\n",
    "# Create a ReAct agent\n",
    "agent = create_react_agent(llm, [retrieve_customer_info])\n",
    "\n",
    "# Invoke the agent on the input\n",
    "messages = agent.invoke({\"messages\": [(\"human\", \"Create a summary of our customer: Peak Performance Co.\")]})\n",
    "print(messages['messages'][-1].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca4611e",
   "metadata": {},
   "source": [
    "## Integrating document loaders\n",
    "\n",
    "### Retrieval Augmented Generation (RAG)\n",
    "\n",
    "Pre-trained language models don't have access to external data sources - their understanding comes purely from their training data. This means that if we require our model to have knowledge that goes beyond its training data, which could be company data or knowledge of more recent world events, we need a way of integrating that data. In RAG, a user query is embedded and used to retrieve the most relevant documents from the database. Then, these documents are added to the model's prompt so that the model has extra context to inform its response.\n",
    "\n",
    "### RAG development steps\n",
    "\n",
    "There are three primary steps to RAG development in LangChain. The first is loading the documents into LangChain with document loaders. Next, is splitting the documents into chunks. Chunks are units of information that we can index and process individually. The last step is encoding and storing the chunks for retrieval, which could utilize a vector database if that meets the needs of the use case. We'll discuss all of these steps throughout the next chapter, but for now, we'll start with document loaders.\n",
    "\n",
    "### LangChain document loaders\n",
    "\n",
    "LangChain document loaders are classes designed to load and configure documents for integration with AI systems. LangChain provides document loader classes for common file types such as CSV and PDFs. There are also additional loaders provided by 3rd parties for managing unique document formats, including Amazon S3 files, Jupyter notebooks, audio transcripts, and many more. In this video, we will practice loading data from three common formats: PDFs, CSVs, and HTML. LangChain has excellent documentation on all of its document loaders, and there's a lot of overlap in syntax, so explore at your leisure!\n",
    "\n",
    "    1 https://python.langchain.com/docs/integrations/document_loaders\n",
    "\n",
    "### PDF document loader\n",
    "\n",
    "There are a few different types of PDF loaders in LangChain, and there is documentation available online for each. In this video, we'll use the PyPDFLoader. We instantiate the PyPDFLoader class, passing in the path to the PDF file we're loading. Finally, we use the .load() method to load the document into memory, and assign the resulting object to the data variable. We can then check the output to confirm that we have loaded it. Note that this document loader requires installation of the pypdf package as a dependency.\n",
    "\n",
    "### HTML document loader\n",
    "\n",
    "Finally, we can load HTML files using the UnstructuredHTMLLoader class. We can access the document's contents, again, with subsetting, and extract the document's metadata with the metadata attribute. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eec670d",
   "metadata": {},
   "source": [
    "## Splitting external data for retrieval\n",
    "\n",
    "### CharacterTextSplitter to split documents\n",
    "\n",
    "Let's start with CharacterTextSplitter. This method splits based on the separator first, then evaluates chunk_size and chunk_overlap to check if it's satisfied. We call CharacterTextSplitter, passing the separator to split on, along with the chunk_size and chunk_overlap. Applying the splitter to the quote with the .split_text() method, and printing the output, we can see that we have a problem: each of these chunks contains more characters than our specified chunk_size. CharacterTextSplitter splits on the separator in an attempt to make chunks smaller than chunk_size, but in this case, splitting on the separator was unable to return chunks below our chunk_size. Let's take a look at a more robust splitting method!\n",
    "\n",
    "### RecursiveCharacterTextSplitter\n",
    "\n",
    "RecursiveCharacterSplitter takes a list of separators to split on, and it works through the list from left to right, splitting the document using each separator in turn, and seeing if these chunks can be combined while remaining under chunk_size. Let's split the quote using the same chunk_size and chunk_overlap.\n",
    "\n",
    "Notice how the length of each chunk varies. The class split by paragraphs first, and found that the chunk size was too big; likewise for sentences. It got to the third separator: splitting words using the space separator, and found that words could be combined into chunks while remaining under the chunk_size character limit. However, some of these chunks are too small to contain meaningful context, but this recursive implementation may work better on larger documents.\n",
    "\n",
    "### RecursiveCharacterTextSplitter with HTML\n",
    "\n",
    "We can also use split other file formats, like HTML. Recall that we can load HTML using UnstructuredHTMLLoader. Defining the splitter is the same, but for splitting documents, we use the .split_documents() method instead of .split_text() to perform the split. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
